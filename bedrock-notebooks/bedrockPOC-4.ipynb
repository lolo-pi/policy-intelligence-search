{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7343603c-6ca0-4443-a316-67c6def0ef25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fd76dee7a54f8792df9dfd014657c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Search:', layout=Layout(width='70%'), placeholder='Search your documents')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c009777b14e5418c8963975598732ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Jurisdiction:', layout=Layout(width='50%'), options=('', 'Colorado', 'California', 'Texa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd707a622ce748878ea9a2be047ae1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Doc Type:', layout=Layout(width='50%'), options=('', 'Guidance Memo', 'Regulation', 'Per…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f231c9bcdc46bf842255351fb8eae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Search Documents', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f198e7742e443cb28540e7f7f57ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Step 1: search, select\n",
    "import boto3\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Initialize the Kendra client\n",
    "kendra_client = boto3.client(\"kendra\")\n",
    "\n",
    "# Function to query the Kendra index with filters\n",
    "def query_kendra(index_id, query_text, jurisdiction=None, doc_type=None):\n",
    "    \"\"\"\n",
    "    Queries AWS Kendra with optional filters for jurisdiction and document type.\n",
    "    \"\"\"\n",
    "    # Build attribute filters dynamically\n",
    "    attribute_filters = []\n",
    "\n",
    "    if jurisdiction:\n",
    "        attribute_filters.append({\n",
    "            \"EqualsTo\": {\n",
    "                \"Key\": \"jurisdiction\",\n",
    "                \"Value\": {\"StringValue\": jurisdiction}\n",
    "            }\n",
    "        })\n",
    "\n",
    "    if doc_type:\n",
    "        attribute_filters.append({\n",
    "            \"EqualsTo\": {\n",
    "                \"Key\": \"doc_type\",\n",
    "                \"Value\": {\"StringValue\": doc_type}\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Combine filters if provided\n",
    "    attribute_filter = {\"AndAllFilters\": attribute_filters} if attribute_filters else None\n",
    "\n",
    "    # Query Kendra\n",
    "    response = kendra_client.query(\n",
    "        IndexId=index_id,\n",
    "        QueryText=query_text,\n",
    "        AttributeFilter=attribute_filter  # Apply filters if present\n",
    "    )\n",
    "\n",
    "    # Extract relevant results\n",
    "    return [\n",
    "        {\n",
    "            \"DocumentTitle\": item.get(\"DocumentTitle\", {}).get(\"Text\", \"No Title\"),\n",
    "            \"DocumentId\": item[\"DocumentId\"],\n",
    "            \"ExcerptText\": item.get(\"ExcerptText\", \"No Excerpt Available\")\n",
    "        }\n",
    "        for item in response[\"ResultItems\"] if \"DocumentId\" in item\n",
    "    ]\n",
    "\n",
    "# UI Elements for Document Search with Filters\n",
    "query_input = widgets.Text(\n",
    "    placeholder=\"Search your documents\",\n",
    "    description=\"Search:\",\n",
    "    layout=widgets.Layout(width=\"70%\")\n",
    ")\n",
    "\n",
    "jurisdiction_dropdown = widgets.Dropdown(\n",
    "    options=[\"\", \"Colorado\", \"California\", \"Texas\"],\n",
    "    description=\"Jurisdiction:\",\n",
    "    layout=widgets.Layout(width=\"50%\")\n",
    ")\n",
    "\n",
    "doc_type_dropdown = widgets.Dropdown(\n",
    "    options=[\"\", \"Guidance Memo\", \"Regulation\", \"Permit\"],\n",
    "    description=\"Doc Type:\",\n",
    "    layout=widgets.Layout(width=\"50%\")\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "select_button = widgets.Button(description=\"Select Documents\")\n",
    "\n",
    "# List to store selected document IDs and titles\n",
    "selected_document_ids = []\n",
    "selected_document_titles = []\n",
    "\n",
    "def on_search_click(change):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        results = query_kendra(\n",
    "            index_id=\"ac2e614a-1a60-4788-921f-439355c5756d\", \n",
    "            query_text=query_input.value,\n",
    "            jurisdiction=jurisdiction_dropdown.value.strip() or None,\n",
    "            doc_type=doc_type_dropdown.value.strip() or None\n",
    "        )\n",
    "        if not results:\n",
    "            display(Markdown(\"**No results found.**\"))\n",
    "            return\n",
    "\n",
    "        display(Markdown(\"**Search Results:**\"))\n",
    "        document_checkboxes = []\n",
    "        for result in results:\n",
    "            # Display result title and excerpt\n",
    "            display(Markdown(f\"**{result['DocumentTitle']}**\\n{result['ExcerptText']}\"))\n",
    "            # Append checkboxes with proper text\n",
    "            checkbox = widgets.Checkbox(description=result['DocumentTitle'], value=False)\n",
    "            checkbox.document_id = result[\"DocumentId\"]  # Store document ID in the checkbox\n",
    "            document_checkboxes.append(checkbox)\n",
    "\n",
    "        # Add the checkboxes and select button\n",
    "        select_button.on_click(lambda x: select_documents(document_checkboxes))\n",
    "        display(widgets.VBox(document_checkboxes))\n",
    "        display(select_button)\n",
    "\n",
    "def select_documents(document_checkboxes):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        selected_document_ids.clear()\n",
    "        selected_document_titles.clear()\n",
    "        for checkbox in document_checkboxes:\n",
    "            if checkbox.value:  # If the checkbox is selected\n",
    "                selected_document_ids.append(checkbox.document_id)\n",
    "                selected_document_titles.append(checkbox.description)\n",
    "\n",
    "        if not selected_document_ids:\n",
    "            display(Markdown(\"**No documents selected.**\"))\n",
    "            return\n",
    "        \n",
    "        # Display the selected documents\n",
    "        display(Markdown(f\"**{len(selected_document_ids)} doc(s) selected. Proceed to step 2.**\"))\n",
    "        display(Markdown(f\"**Doc Titles:** {', '.join(selected_document_titles)}\"))\n",
    "\n",
    "# Display UI\n",
    "search_button = widgets.Button(description=\"Search Documents\")\n",
    "search_button.on_click(on_search_click)\n",
    "\n",
    "display(query_input, jurisdiction_dropdown, doc_type_dropdown, search_button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508f6871-f9fa-412f-88aa-f2fd4bff87d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a875769a346e4de2bad77d93325ece4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=500, description='Chunk Size:', max=2000, min=100, step=100), Button(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Fetch, Chunk and Map Chunk Source (new delete above if works)\n",
    "import boto3\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "import re\n",
    "import io\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# Function to fetch document text from S3\n",
    "def fetch_document_text(document_uri):\n",
    "    \"\"\"\n",
    "    Fetches document content from S3, dynamically handling multiple buckets\n",
    "    and supporting both PDFs and text files.\n",
    "    \"\"\"\n",
    "    # Extract bucket name and object key from S3 URI\n",
    "    match = re.match(r\"s3://([^/]+)/(.*)\", document_uri)  # Extract bucket and key\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid S3 URI format: {document_uri}\")\n",
    "\n",
    "    bucket_name, object_key = match.groups()\n",
    "    print(f\"Fetching from S3: Bucket={bucket_name}, Key={object_key}\")  # Debugging info\n",
    "\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "        content = response[\"Body\"].read()\n",
    "\n",
    "        # Handle PDFs\n",
    "        if object_key.endswith(\".pdf\"):\n",
    "            pdf_reader = PdfReader(io.BytesIO(content))\n",
    "            text = \"\\n\".join([page.extract_text() for page in pdf_reader.pages if page.extract_text()])\n",
    "            return text\n",
    "\n",
    "        return content.decode(\"utf-8\")  # Handle text files\n",
    "    except s3_client.exceptions.NoSuchKey:\n",
    "        raise ValueError(f\"Error: The file '{object_key}' does not exist in bucket '{bucket_name}'.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Unexpected error fetching document: {e}\")\n",
    "\n",
    "# Function to chunk document into smaller pieces\n",
    "def chunk_document(text, chunk_size=500):\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# UI for Ingestion and Optimization\n",
    "chunk_size_slider = widgets.IntSlider(\n",
    "    value=500, min=100, max=2000, step=100, description=\"Chunk Size:\"\n",
    ")\n",
    "ingest_button = widgets.Button(description=\"Ingest Documents\")\n",
    "ingestion_output = widgets.Output()\n",
    "\n",
    "# Step 2: Store all document chunks properly\n",
    "all_chunks = []\n",
    "chunk_doc_map = []  # Stores mapping of chunk -> document title\n",
    "\n",
    "def ingest_documents(change):\n",
    "    global all_chunks, chunk_doc_map  # Store chunks and their source docs\n",
    "    with ingestion_output:\n",
    "        ingestion_output.clear_output()\n",
    "        if not selected_document_ids:\n",
    "            display(Markdown(\"**No documents selected. Please complete Step 1.**\"))\n",
    "            return\n",
    "\n",
    "        all_chunks = []  # Reset chunks\n",
    "        chunk_doc_map = []  # Reset mapping\n",
    "\n",
    "        for doc_id in selected_document_ids:\n",
    "            try:\n",
    "                doc_text = fetch_document_text(doc_id)\n",
    "                chunks = chunk_document(doc_text, chunk_size=chunk_size_slider.value)\n",
    "\n",
    "                # Store chunks and associate them with their document\n",
    "                all_chunks.extend(chunks)\n",
    "                chunk_doc_map.extend([doc_id] * len(chunks))  # Map each chunk to its document\n",
    "\n",
    "                display(Markdown(f\"Successfully processed document: **{doc_id}**\"))\n",
    "            except ValueError as e:\n",
    "                display(Markdown(f\"**Error:** {e}\"))\n",
    "\n",
    "        if all_chunks:\n",
    "            display(Markdown(f\"**{len(all_chunks)} document chunks stored. Proceed to Step 3.**\"))\n",
    "            \n",
    "            # Debugging: Print distinct document sources\n",
    "            unique_sources = set(chunk_doc_map)\n",
    "            print(\"Debugging: Unique document sources used:\")\n",
    "            for source in unique_sources:\n",
    "                print(source)\n",
    "\n",
    "        else:\n",
    "            display(Markdown(\"**No valid text extracted.**\"))\n",
    "\n",
    "ingest_button.on_click(ingest_documents)\n",
    "display(widgets.VBox([chunk_size_slider, ingest_button, ingestion_output]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c30adcdf-a0d8-4665-b841-426f0e7c57c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6630fb7ae85c4fc1925f06f32b84bba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Chat with Your Documents (Enhanced Source Tracking)</h3>'), HBox(children=(Text…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f15da94afe46758a9c31c89dad5a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3Enhanced Interactive Chat Interface with Source Tracking\n",
    "import boto3\n",
    "import json\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import ipywidgets as widgets\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "bedrock_runtime_client = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "# Function to invoke the Claude model\n",
    "def invoke_claude_model(context, question, max_output_tokens=6000):\n",
    "    try:\n",
    "        # Build the messages list for Claude\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\nQuestion: {question}\"}\n",
    "        ]\n",
    "\n",
    "        response = bedrock_runtime_client.invoke_model(\n",
    "            modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "            body=json.dumps({\n",
    "                \"messages\": messages,\n",
    "                \"max_tokens\": max_output_tokens,\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "            })\n",
    "        )\n",
    "\n",
    "        response_body = response[\"body\"].read().decode(\"utf-8\")\n",
    "        result = json.loads(response_body)\n",
    "\n",
    "        if \"content\" in result and isinstance(result[\"content\"], list):\n",
    "            return \" \".join([item[\"text\"] for item in result[\"content\"] if item[\"type\"] == \"text\"])\n",
    "        else:\n",
    "            raise ValueError(\"Claude API response does not contain valid 'content'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to find the most relevant chunks for a question\n",
    "def find_relevant_chunks(question, chunks, chunk_doc_map, num_chunks=15):\n",
    "    \"\"\"\n",
    "    Finds the most relevant chunks based on keyword overlap with the question and labels them with source documents.\n",
    "    Also returns detailed statistics about which sources were used and how many chunks from each.\n",
    "    \"\"\"\n",
    "    keywords = set(question.lower().split())  # Extract words from the user's question\n",
    "    scored_chunks = []\n",
    "    selected_sources = set()\n",
    "\n",
    "    # Score chunks based on keyword matches and retain document references\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_words = set(chunk.lower().split())\n",
    "        match_count = len(keywords.intersection(chunk_words))  # Count overlapping words\n",
    "        if match_count > 0:\n",
    "            doc_source = chunk_doc_map[i]  # Get original document source\n",
    "            scored_chunks.append((chunk, match_count, doc_source, i))  # Store chunk, score, source, and index\n",
    "            selected_sources.add(doc_source)\n",
    "\n",
    "    # Sort by relevance (highest matches first)\n",
    "    scored_chunks.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Take top N chunks\n",
    "    top_chunks = scored_chunks[:num_chunks]\n",
    "    \n",
    "    # Extract sources and chunks used with indices\n",
    "    used_chunks = [(chunk[0], chunk[2], chunk[3]) for chunk in top_chunks]  # (chunk_text, source, index)\n",
    "    source_counter = Counter([chunk[2] for chunk in top_chunks])  # Count chunks per source\n",
    "    \n",
    "    # Create formatted context chunks\n",
    "    relevant_chunks = [f\"[Source: {chunk[2]}] {chunk[0]}\" for chunk in top_chunks]\n",
    "\n",
    "    # Fallback: If no relevant chunks found, select evenly distributed chunks\n",
    "    if not relevant_chunks and chunks:\n",
    "        step = max(1, len(chunks) // num_chunks)\n",
    "        indices = [i for i in range(0, len(chunks), step)[:num_chunks]]\n",
    "        relevant_chunks = [f\"[Source: {chunk_doc_map[i]}] {chunks[i]}\" for i in indices]\n",
    "        source_counter = Counter([chunk_doc_map[i] for i in indices])\n",
    "        used_chunks = [(chunks[i], chunk_doc_map[i], i) for i in indices]\n",
    "        selected_sources = set([chunk_doc_map[i] for i in indices])\n",
    "\n",
    "    return relevant_chunks, list(selected_sources), source_counter, used_chunks\n",
    "\n",
    "def create_enhanced_chat_interface():\n",
    "    \"\"\"\n",
    "    Creates and returns an improved chat interface with better source tracking.\n",
    "    \"\"\"\n",
    "    # Create widgets for interactive chat interface\n",
    "    question_input = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Ask a question about your documents',\n",
    "        description='Question:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "\n",
    "    submit_button = widgets.Button(\n",
    "        description='Ask',\n",
    "        button_style='primary',\n",
    "        tooltip='Submit your question'\n",
    "    )\n",
    "\n",
    "    # Create output widget with scrolling and increased height\n",
    "    chat_output = widgets.Output(\n",
    "        layout=widgets.Layout(\n",
    "            height='550px',  # Increased height for source statistics\n",
    "            width='95%',     # Wider display\n",
    "            overflow='auto',  # Enable scrolling\n",
    "            border='1px solid #ddd',\n",
    "            padding='10px'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Handler for submit button click\n",
    "    def on_submit_question(b):\n",
    "        global all_chunks, chunk_doc_map\n",
    "        \n",
    "        question = question_input.value.strip()\n",
    "        \n",
    "        if not question:\n",
    "            return\n",
    "        \n",
    "        if not all_chunks:\n",
    "            with chat_output:\n",
    "                display(Markdown(\"**Error:** No document chunks available. Please run Step 2 first.\"))\n",
    "            return\n",
    "        \n",
    "        with chat_output:\n",
    "            # Clear previous output and show the question\n",
    "            chat_output.clear_output()\n",
    "            display(Markdown(f\"**User Question:** {question}\"))\n",
    "            display(Markdown(\"*Processing...*\"))\n",
    "            \n",
    "            # Find relevant chunks and get AI response with enhanced source tracking\n",
    "            context_chunks, selected_sources, source_counter, used_chunks = find_relevant_chunks(\n",
    "                question, all_chunks, chunk_doc_map, num_chunks=35\n",
    "            )\n",
    "            context = \" \".join(context_chunks)\n",
    "            \n",
    "            # Debug info\n",
    "            print(f\"Found {len(context_chunks)} relevant chunks from {len(selected_sources)} documents\")\n",
    "            \n",
    "            # Invoke Claude model and display response\n",
    "            chat_output.clear_output()\n",
    "            display(Markdown(f\"**User Question:** {question}\"))\n",
    "            \n",
    "            response = invoke_claude_model(context, question, max_output_tokens=6000)\n",
    "            \n",
    "            if response:\n",
    "                # Format the response with HTML for better readability\n",
    "                display(HTML(f\"<div style='white-space: pre-wrap; margin: 10px 0;'><b>AI Response:</b><br>{response}</div>\"))\n",
    "                \n",
    "                if selected_sources:\n",
    "                    # Format detailed source usage statistics\n",
    "                    display(HTML(\"<b>Source Usage Statistics:</b>\"))\n",
    "                    display(HTML(\"<div style='margin: 10px 0; padding: 10px; background-color: #f8f8f8; border-radius: 5px;'>\"))\n",
    "                    \n",
    "                    # Display table with source usage information\n",
    "                    table_html = \"\"\"<table style='width:100%; border-collapse: collapse;'>\n",
    "                        <tr style='background-color: #e0e0e0;'>\n",
    "                            <th style='padding: 8px; text-align: left; border: 1px solid #ddd;'>Source</th>\n",
    "                            <th style='padding: 8px; text-align: center; border: 1px solid #ddd;'>Chunks Used</th>\n",
    "                            <th style='padding: 8px; text-align: center; border: 1px solid #ddd;'>% of Context</th>\n",
    "                        </tr>\"\"\"\n",
    "                    \n",
    "                    # Calculate total chunks used\n",
    "                    total_chunks = sum(source_counter.values())\n",
    "                    \n",
    "                    # Add rows for each source\n",
    "                    for source, count in source_counter.most_common():\n",
    "                        percentage = (count / total_chunks) * 100\n",
    "                        table_html += f\"\"\"<tr>\n",
    "                            <td style='padding: 8px; border: 1px solid #ddd;'>{source}</td>\n",
    "                            <td style='padding: 8px; text-align: center; border: 1px solid #ddd;'>{count}</td>\n",
    "                            <td style='padding: 8px; text-align: center; border: 1px solid #ddd;'>{percentage:.1f}%</td>\n",
    "                        </tr>\"\"\"\n",
    "                    \n",
    "                    table_html += \"</table>\"\n",
    "                    display(HTML(table_html))\n",
    "                    display(HTML(\"</div>\"))\n",
    "                    \n",
    "                    # Add option to show all available sources\n",
    "                    show_all_button = widgets.Button(\n",
    "                        description=\"Show All Available Sources\",\n",
    "                        button_style=\"info\",\n",
    "                        layout=widgets.Layout(width='auto')\n",
    "                    )\n",
    "                    \n",
    "                    sources_output = widgets.Output()\n",
    "                    \n",
    "                    def on_show_all(b):\n",
    "                        with sources_output:\n",
    "                            sources_output.clear_output()\n",
    "                            unique_sources = set(chunk_doc_map)\n",
    "                            display(HTML(\"<b>All Available Sources:</b>\"))\n",
    "                            for src in unique_sources:\n",
    "                                has_chunks = src in source_counter\n",
    "                                style = \"color: green;\" if has_chunks else \"color: #888;\"\n",
    "                                count_text = f\"({source_counter[src]} chunks)\" if has_chunks else \"(not used)\"\n",
    "                                display(HTML(f\"<div style='margin-left: 20px; {style}'>• {src} {count_text}</div>\"))\n",
    "                    \n",
    "                    show_all_button.on_click(on_show_all)\n",
    "                    display(show_all_button)\n",
    "                    display(sources_output)\n",
    "            else:\n",
    "                display(Markdown(\"**Error:** Failed to get a response from the AI model.\"))\n",
    "        \n",
    "        # Clear the input field for the next question\n",
    "        question_input.value = ''\n",
    "\n",
    "    # Connect the button to the handler function\n",
    "    submit_button.on_click(on_submit_question)\n",
    "\n",
    "    # Handle Enter key in the text input using the modern pattern\n",
    "    question_input.observe(lambda change: on_submit_question(None) \n",
    "                         if change['type'] == 'change' and change['name'] == 'value' and change.get('new', '').endswith('\\n') \n",
    "                         else None, 'value')\n",
    "\n",
    "    # Return the assembled interface\n",
    "    return widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Chat with Your Documents (Enhanced Source Tracking)</h3>\"),\n",
    "        widgets.HBox([question_input, submit_button]),\n",
    "        chat_output\n",
    "    ])\n",
    "\n",
    "# To use this in a notebook, run:\n",
    "display(create_enhanced_chat_interface())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2ce161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f307653ef30465f90b5a6d8108a8113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>📚 Chat with Your Documents (Enhanced Source Diversity)</h3>'), HBox(children=(T…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f330240d289745c8bc40742d1c398f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3.1: Enhanced Source Diversity Version\n",
    "import boto3\n",
    "import json\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import ipywidgets as widgets\n",
    "from collections import Counter\n",
    "\n",
    "# Function to invoke the Claude model with source diversity instruction\n",
    "def invoke_claude_model_diverse(context, question, max_output_tokens=6000):\n",
    "    \"\"\"\n",
    "    Invokes Claude with explicit instruction to use all available sources.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add explicit instruction to consider all sources\n",
    "        instruction = \"Please consider information from ALL available sources in the provided context when answering. Make sure to draw from as many different sources as possible for your response.\"\n",
    "        \n",
    "        # Build the messages list for Claude with the diversity instruction\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\n{instruction}\\n\\nQuestion: {question}\"}\n",
    "        ]\n",
    "\n",
    "        response = bedrock_runtime_client.invoke_model(\n",
    "            modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "            body=json.dumps({\n",
    "                \"messages\": messages,\n",
    "                \"max_tokens\": max_output_tokens,\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "            })\n",
    "        )\n",
    "\n",
    "        response_body = response[\"body\"].read().decode(\"utf-8\")\n",
    "        result = json.loads(response_body)\n",
    "\n",
    "        if \"content\" in result and isinstance(result[\"content\"], list):\n",
    "            return \" \".join([item[\"text\"] for item in result[\"content\"] if item[\"type\"] == \"text\"])\n",
    "        else:\n",
    "            raise ValueError(\"Claude API response does not contain valid 'content'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Enhanced function to find relevant chunks with maximized source diversity\n",
    "def find_relevant_chunks_diverse(question, chunks, chunk_doc_map, num_chunks=50):\n",
    "    \"\"\"\n",
    "    Finds the most relevant chunks with enhanced source diversity features.\n",
    "    1. Scores chunks based on keyword overlap with question\n",
    "    2. Applies source diversity boosting\n",
    "    3. Ensures representation from all available sources\n",
    "    4. Returns detailed source statistics\n",
    "    \"\"\"\n",
    "    keywords = set(question.lower().split())  # Extract words from the user's question\n",
    "    scored_chunks = []\n",
    "    all_sources = set(chunk_doc_map)  # All available sources\n",
    "    print(f\"Available sources: {len(all_sources)}\")\n",
    "    \n",
    "    # PHASE 1: Score chunks based on keyword matches\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_words = set(chunk.lower().split())\n",
    "        match_count = len(keywords.intersection(chunk_words))  # Count overlapping words\n",
    "        if match_count > 0:  # Only consider chunks with at least one keyword match\n",
    "            doc_source = chunk_doc_map[i]  # Get original document source\n",
    "            scored_chunks.append((chunk, match_count, doc_source, i))  # Store chunk, score, source, and index\n",
    "    \n",
    "    # If no chunks matched keywords, use all chunks with minimal scores\n",
    "    if not scored_chunks:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            doc_source = chunk_doc_map[i]\n",
    "            scored_chunks.append((chunk, 0.1, doc_source, i))  # Minimal score\n",
    "    \n",
    "    # PHASE 2: Apply source diversity boosting\n",
    "    # Count how many chunks we have from each source\n",
    "    source_counts = Counter([src for _, _, src, _ in scored_chunks])\n",
    "    \n",
    "    # Calculate diversity boost for each chunk - less common sources get bigger boost\n",
    "    max_count = max(source_counts.values()) if source_counts else 1\n",
    "    diversity_boosted_chunks = []\n",
    "    \n",
    "    for chunk, score, source, idx in scored_chunks:\n",
    "        # Apply diversity boost - give bonus to sources with fewer chunks\n",
    "        diversity_boost = 1.0 + (max_count / (source_counts[source])) * 0.5\n",
    "        new_score = score * diversity_boost\n",
    "        diversity_boosted_chunks.append((chunk, new_score, source, idx))\n",
    "    \n",
    "    # Sort by boosted score\n",
    "    diversity_boosted_chunks.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # PHASE 3: Ensure representation from all sources\n",
    "    # First pass: select one chunk from each source\n",
    "    ensured_chunks = []\n",
    "    source_included = set()  # Track sources we've included\n",
    "    \n",
    "    # First ensure every source gets at least one chunk if possible\n",
    "    for source in all_sources:\n",
    "        # Find the highest scored chunk for this source\n",
    "        source_chunks = [(chunk, score, src, idx) for chunk, score, src, idx \n",
    "                         in diversity_boosted_chunks if src == source]\n",
    "        \n",
    "        if source_chunks:  # If we have chunks from this source\n",
    "            best_chunk = max(source_chunks, key=lambda x: x[1])  # Get highest scored chunk\n",
    "            ensured_chunks.append(best_chunk)\n",
    "            source_included.add(source)\n",
    "    \n",
    "    # Then fill remaining slots with best chunks\n",
    "    remaining_chunks = [c for c in diversity_boosted_chunks \n",
    "                        if c not in ensured_chunks]\n",
    "    remaining_chunks.sort(key=lambda x: x[1], reverse=True)  # Sort by score\n",
    "    \n",
    "    # Fill up to num_chunks\n",
    "    final_chunks = ensured_chunks + remaining_chunks[:num_chunks - len(ensured_chunks)]\n",
    "    \n",
    "    # Sort by relevance score for final ordering\n",
    "    final_chunks.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Generate statistics and prepare output\n",
    "    used_source_counter = Counter([chunk[2] for chunk in final_chunks])\n",
    "    selected_sources = list(used_source_counter.keys())\n",
    "    used_chunks = [(chunk[0], chunk[2], chunk[3]) for chunk in final_chunks]  # (text, source, index)\n",
    "    \n",
    "    # Create formatted context chunks\n",
    "    relevant_chunks = [f\"[Source: {chunk[2]}] {chunk[0]}\" for chunk in final_chunks]\n",
    "    \n",
    "    print(f\"Sources representation: {len(selected_sources)}/{len(all_sources)} sources\")\n",
    "    for source, count in used_source_counter.most_common():\n",
    "        print(f\"  - {source}: {count} chunks\")\n",
    "        \n",
    "    return relevant_chunks, selected_sources, used_source_counter, used_chunks\n",
    "\n",
    "# Create our enhanced chat interface with source diversity\n",
    "def create_diverse_chat_interface():\n",
    "    \"\"\"\n",
    "    Creates and returns an improved chat interface with maximized source diversity.\n",
    "    \"\"\"\n",
    "    # Create widgets for interactive chat interface\n",
    "    question_input = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Ask a question about your documents',\n",
    "        description='Question:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "\n",
    "    submit_button = widgets.Button(\n",
    "        description='Ask',\n",
    "        button_style='primary',\n",
    "        tooltip='Submit your question'\n",
    "    )\n",
    "    \n",
    "    # Chunk count slider for fine-tuning\n",
    "    chunk_slider = widgets.IntSlider(\n",
    "        value=50, \n",
    "        min=15, \n",
    "        max=100, \n",
    "        step=5, \n",
    "        description='Max Chunks:',\n",
    "        layout=widgets.Layout(width='50%'),\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    # Create output widget with scrolling and increased height\n",
    "    chat_output = widgets.Output(\n",
    "        layout=widgets.Layout(\n",
    "            height='650px',  # Increased height for source statistics\n",
    "            width='95%',     # Wider display\n",
    "            overflow='auto',  # Enable scrolling\n",
    "            border='1px solid #ddd',\n",
    "            padding='10px'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Handler for submit button click\n",
    "    def on_submit_question(b):\n",
    "        global all_chunks, chunk_doc_map\n",
    "        \n",
    "        question = question_input.value.strip()\n",
    "        \n",
    "        if not question:\n",
    "            return\n",
    "        \n",
    "        if not all_chunks:\n",
    "            with chat_output:\n",
    "                display(Markdown(\"**Error:** No document chunks available. Please run Step 2 first.\"))\n",
    "            return\n",
    "        \n",
    "        with chat_output:\n",
    "            # Clear previous output and show the question\n",
    "            chat_output.clear_output()\n",
    "            display(Markdown(f\"**User Question:** {question}\"))\n",
    "            display(Markdown(\"*Processing with source diversity optimization...*\"))\n",
    "            \n",
    "            # Use enhanced diverse chunk finder\n",
    "            context_chunks, selected_sources, source_counter, used_chunks = find_relevant_chunks_diverse(\n",
    "                question, all_chunks, chunk_doc_map, num_chunks=chunk_slider.value\n",
    "            )\n",
    "            context = \" \".join(context_chunks)\n",
    "            \n",
    "            # Invoke Claude model and display response\n",
    "            chat_output.clear_output()\n",
    "            display(Markdown(f\"**User Question:** {question}\"))\n",
    "            display(Markdown(\"*Generating answer with source diversity optimization...*\"))\n",
    "            \n",
    "            response = invoke_claude_model_diverse(context, question, max_output_tokens=6000)\n",
    "            \n",
    "            if response:\n",
    "                # Clear processing messages\n",
    "                chat_output.clear_output()\n",
    "                display(Markdown(f\"**User Question:** {question}\"))\n",
    "                \n",
    "                # Format the response with HTML for better readability\n",
    "                display(HTML(f\"<div style='white-space: pre-wrap; margin: 10px 0;'><b>AI Response:</b><br>{response}</div>\"))\n",
    "                \n",
    "                # Show source diversity metrics\n",
    "                all_sources = set(chunk_doc_map)\n",
    "                source_coverage = (len(selected_sources) / len(all_sources)) * 100 if all_sources else 0\n",
    "                \n",
    "                display(HTML(f\"<div style='margin: 15px 0;'>\"\n",
    "                           f\"<b>Source Coverage:</b> {len(selected_sources)}/{len(all_sources)} sources \"\n",
    "                           f\"({source_coverage:.1f}%)</div>\"))\n",
    "                \n",
    "                if selected_sources:\n",
    "                    # Format detailed source usage statistics\n",
    "                    display(HTML(\"<b>Source Usage Statistics:</b>\"))\n",
    "                    display(HTML(\"<div style='margin: 10px 0; padding: 10px; background-color: #f8f8f8; border-radius: 5px;'>\"))\n",
    "                    \n",
    "                    # Display table with source usage information\n",
    "                    table_html = \"\"\"<table style='width:100%; border-collapse: collapse;'>\n",
    "                        <tr style='background-color: #e0e0e0;'>\n",
    "                            <th style='padding: 8px; text-align: left; border: 1px solid #ddd;'>Source</th>\n",
    "                            <th style='padding: 8px; text-align: center; border: 1px solid #ddd;'>Chunks Used</th>\n",
    "                            <th style='padding: 8px; text-align: center; border: 1px solid #ddd;'>% of Context</th>\n",
    "                        </tr>\"\"\"\n",
    "                    \n",
    "                    # Calculate total chunks used\n",
    "                    total_chunks = sum(source_counter.values())\n",
    "                    \n",
    "                    # Add rows for each source\n",
    "                    for source, count in source_counter.most_common():\n",
    "                        percentage = (count / total_chunks) * 100\n",
    "                        table_html += f\"\"\"<tr>\n",
    "                            <td style='padding: 8px; border: 1px solid #ddd;'>{source}</td>\n",
    "                            <td style='padding: 8px; text-align: center; border: 1px solid #ddd;'>{count}</td>\n",
    "                            <td style='padding: 8px; text-align: center; border: 1px solid #ddd;'>{percentage:.1f}%</td>\n",
    "                        </tr>\"\"\"\n",
    "                    \n",
    "                    table_html += \"</table>\"\n",
    "                    display(HTML(table_html))\n",
    "                    display(HTML(\"</div>\"))\n",
    "                    \n",
    "                    # Add option to show all available sources\n",
    "                    show_all_button = widgets.Button(\n",
    "                        description=\"Show All Available Sources\",\n",
    "                        button_style=\"info\",\n",
    "                        layout=widgets.Layout(width='auto')\n",
    "                    )\n",
    "                    \n",
    "                    sources_output = widgets.Output()\n",
    "                    \n",
    "                    def on_show_all(b):\n",
    "                        with sources_output:\n",
    "                            sources_output.clear_output()\n",
    "                            unique_sources = set(chunk_doc_map)\n",
    "                            display(HTML(\"<b>All Available Sources:</b>\"))\n",
    "                            for src in unique_sources:\n",
    "                                has_chunks = src in source_counter\n",
    "                                style = \"color: green; font-weight: bold;\" if has_chunks else \"color: #888;\"\n",
    "                                count_text = f\"({source_counter[src]} chunks)\" if has_chunks else \"(not used)\"\n",
    "                                display(HTML(f\"<div style='margin-left: 20px; {style}'>• {src} {count_text}</div>\"))\n",
    "                    \n",
    "                    show_all_button.on_click(on_show_all)\n",
    "                    display(show_all_button)\n",
    "                    display(sources_output)\n",
    "            else:\n",
    "                display(Markdown(\"**Error:** Failed to get a response from the AI model.\"))\n",
    "        \n",
    "        # Clear the input field for the next question\n",
    "        question_input.value = ''\n",
    "\n",
    "    # Connect the button to the handler function\n",
    "    submit_button.on_click(on_submit_question)\n",
    "\n",
    "    # Handle Enter key in the text input using the modern pattern\n",
    "    question_input.observe(lambda change: on_submit_question(None) \n",
    "                         if change['type'] == 'change' and change['name'] == 'value' and change.get('new', '').endswith('\\n') \n",
    "                         else None, 'value')\n",
    "\n",
    "    # Return the assembled interface\n",
    "    return widgets.VBox([\n",
    "        widgets.HTML(\"<h3>📚 Chat with Your Documents (Enhanced Source Diversity)</h3>\"),\n",
    "        widgets.HBox([question_input, submit_button]),\n",
    "        widgets.HBox([chunk_slider]),\n",
    "        chat_output\n",
    "    ])\n",
    "\n",
    "# Display the enhanced interface\n",
    "display(create_diverse_chat_interface())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94828b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f7d193f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0417556e7049b0a0fa7b5cf7bf3753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>⚖️ Chat with Your Documents (Balanced Source Distribution)</h3>'), HBox(childre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d240e848efa84a6a82b0d3e102a3f354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebaa545a838f4bad9e01aef900c1c6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054de6f7f8e5468095fbb95487dbdfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#step 3.2 improved and balanced source diversity, invoke model chat with your docs\n",
    "import boto3\n",
    "import json\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import ipywidgets as widgets\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "bedrock_runtime_client = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "# Function to invoke the Claude model with balanced sources\n",
    "def invoke_claude_model_balanced(context, question, max_output_tokens=6000):\n",
    "    \"\"\"\n",
    "    Invokes Claude with explicit instruction to use balanced information from all sources.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add explicit instruction about balanced source usage\n",
    "        instruction = \"Please use a balanced approach drawing from ALL available sources in the provided context. Give approximately equal weight to each different source document when crafting your response.\"\n",
    "        \n",
    "        # Build the messages list for Claude with the balance instruction\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\n{instruction}\\n\\nQuestion: {question}\"}\n",
    "        ]\n",
    "\n",
    "        response = bedrock_runtime_client.invoke_model(\n",
    "            modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "            body=json.dumps({\n",
    "                \"messages\": messages,\n",
    "                \"max_tokens\": max_output_tokens,\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "            })\n",
    "        )\n",
    "\n",
    "        response_body = response[\"body\"].read().decode(\"utf-8\")\n",
    "        result = json.loads(response_body)\n",
    "\n",
    "        if \"content\" in result and isinstance(result[\"content\"], list):\n",
    "            return \" \".join([item[\"text\"] for item in result[\"content\"] if item[\"type\"] == \"text\"])\n",
    "        else:\n",
    "            raise ValueError(\"Claude API response does not contain valid 'content'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Truly balanced chunk selection algorithm\n",
    "def find_relevant_chunks_balanced(question, chunks, chunk_doc_map, num_chunks=50, diversity_weight=0.7):\n",
    "    \"\"\"\n",
    "    Finds relevant chunks with a truly balanced approach across sources:\n",
    "    1. Initial scoring based on keyword relevance\n",
    "    2. Ensures one chunk from each source\n",
    "    3. Uses round-robin selection to ensure balanced representation\n",
    "    4. Applies stronger diversity weighting\n",
    "    \"\"\"\n",
    "    keywords = set(question.lower().split())  # Extract words from the user's question\n",
    "    scored_chunks = []\n",
    "    all_sources = list(set(chunk_doc_map))  # All available sources\n",
    "    print(f\"Available sources: {len(all_sources)}\")\n",
    "    \n",
    "    # PHASE 1: Initial scoring of chunks based on keyword matches\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_words = set(chunk.lower().split())\n",
    "        match_count = len(keywords.intersection(chunk_words))  # Count overlapping words\n",
    "        doc_source = chunk_doc_map[i]  # Get original document source\n",
    "        scored_chunks.append((chunk, match_count, doc_source, i))  # Store chunk, score, source, and index\n",
    "    \n",
    "    # PHASE 2: Group chunks by source\n",
    "    source_chunks = defaultdict(list)\n",
    "    for chunk_data in scored_chunks:\n",
    "        chunk, score, source, idx = chunk_data\n",
    "        source_chunks[source].append((chunk, score, source, idx))\n",
    "    \n",
    "    # Sort chunks within each source by relevance score\n",
    "    for source in source_chunks:\n",
    "        source_chunks[source].sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # PHASE 3: Calculate how many chunks to take from each source for balanced representation\n",
    "    source_count = len(all_sources)\n",
    "    chunks_per_source = max(1, num_chunks // source_count)\n",
    "    \n",
    "    # Ensure remaining chunks are distributed evenly\n",
    "    remaining = num_chunks - (chunks_per_source * source_count)\n",
    "    extra_chunks = [1] * remaining + [0] * (source_count - remaining)\n",
    "    \n",
    "    # PHASE 4: Select chunks using round-robin with adjusted allocations\n",
    "    final_chunks = []\n",
    "    \n",
    "    # First, ensure every source gets its fair allocation\n",
    "    for i, source in enumerate(all_sources):\n",
    "        # Calculate how many chunks to take from this source\n",
    "        allocation = chunks_per_source + extra_chunks[i]\n",
    "        \n",
    "        # Get the top N chunks from this source (or as many as available)\n",
    "        source_selection = source_chunks[source][:allocation]\n",
    "        final_chunks.extend(source_selection)\n",
    "    \n",
    "    # PHASE 5: If we still have slots to fill, take the most relevant remaining chunks\n",
    "    if len(final_chunks) < num_chunks:\n",
    "        # Collect all remaining chunks\n",
    "        remaining_chunks = []\n",
    "        for source in all_sources:\n",
    "            used = len([c for c in final_chunks if c[2] == source])\n",
    "            remaining_chunks.extend(source_chunks[source][used:])\n",
    "        \n",
    "        # Apply a stronger diversity boost to remaining chunks\n",
    "        boosted_remaining = []\n",
    "        for chunk, score, source, idx in remaining_chunks:\n",
    "            # Count how many chunks we already took from this source\n",
    "            already_used = len([c for c in final_chunks if c[2] == source])\n",
    "            \n",
    "            # Stronger inverse scaling - sources with fewer chunks get more boost\n",
    "            diversity_penalty = already_used ** 2  # Quadratic penalty\n",
    "            adjusted_score = score - (diversity_penalty * diversity_weight)\n",
    "            boosted_remaining.append((chunk, adjusted_score, source, idx))\n",
    "        \n",
    "        # Sort and add remaining chunks until we reach the limit\n",
    "        boosted_remaining.sort(key=lambda x: x[1], reverse=True)\n",
    "        final_chunks.extend(boosted_remaining[:num_chunks - len(final_chunks)])\n",
    "    \n",
    "    # Generate statistics and prepare output\n",
    "    used_source_counter = Counter([chunk[2] for chunk in final_chunks])\n",
    "    selected_sources = list(used_source_counter.keys())\n",
    "    used_chunks = [(chunk[0], chunk[2], chunk[3]) for chunk in final_chunks]  # (text, source, index)\n",
    "    \n",
    "    # Create formatted context chunks\n",
    "    relevant_chunks = [f\"[Source: {chunk[2]}] {chunk[0]}\" for chunk in final_chunks]\n",
    "    \n",
    "    print(f\"Sources representation: {len(selected_sources)}/{len(all_sources)} sources\")\n",
    "    for source, count in used_source_counter.most_common():\n",
    "        print(f\"  - {source}: {count} chunks\")\n",
    "        \n",
    "    return relevant_chunks, selected_sources, used_source_counter, used_chunks\n",
    "\n",
    "# Create balanced chat interface\n",
    "def create_balanced_chat_interface():\n",
    "    \"\"\"\n",
    "    Creates and returns an improved chat interface with truly balanced source distribution.\n",
    "    \"\"\"\n",
    "    # Create widgets for interactive chat interface\n",
    "    question_input = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Ask a question about your documents',\n",
    "        description='Question:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "\n",
    "    submit_button = widgets.Button(\n",
    "        description='Ask',\n",
    "        button_style='primary',\n",
    "        tooltip='Submit your question'\n",
    "    )\n",
    "    \n",
    "    # Chunk count and diversity weight sliders\n",
    "    chunk_slider = widgets.IntSlider(\n",
    "        value=50, \n",
    "        min=15, \n",
    "        max=100, \n",
    "        step=5, \n",
    "        description='Max Chunks:',\n",
    "        layout=widgets.Layout(width='50%'),\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    diversity_slider = widgets.FloatSlider(\n",
    "        value=0.7,\n",
    "        min=0.1,\n",
    "        max=1.0,\n",
    "        step=0.1,\n",
    "        description='Diversity Weight:',\n",
    "        layout=widgets.Layout(width='50%'),\n",
    "        style={'description_width': 'initial'},\n",
    "        tooltip='Higher value means more balanced source distribution'\n",
    "    )\n",
    "\n",
    "    # Create output widget with scrolling and increased height\n",
    "    chat_output = widgets.Output(\n",
    "        layout=widgets.Layout(\n",
    "            height='650px',  # Increased height for source statistics\n",
    "            width='95%',     # Wider display\n",
    "            overflow='auto',  # Enable scrolling\n",
    "            border='1px solid #ddd',\n",
    "            padding='10px'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Handler for submit button click\n",
    "    def on_submit_question(b):\n",
    "        global all_chunks, chunk_doc_map\n",
    "        \n",
    "        question = question_input.value.strip()\n",
    "        \n",
    "        if not question:\n",
    "            return\n",
    "        \n",
    "        if not all_chunks:\n",
    "            with chat_output:\n",
    "                display(Markdown(\"**Error:** No document chunks available. Please run Step 2 first.\"))\n",
    "            return\n",
    "        \n",
    "        with chat_output:\n",
    "            # Clear previous output and show the question\n",
    "            chat_output.clear_output()\n",
    "            display(Markdown(f\"**User Question:** {question}\"))\n",
    "            display(Markdown(\"*Processing with balanced source distribution...*\"))\n",
    "            \n",
    "            # Use balanced chunk finder\n",
    "            context_chunks, selected_sources, source_counter, used_chunks = find_relevant_chunks_balanced(\n",
    "                question, all_chunks, chunk_doc_map, \n",
    "                num_chunks=chunk_slider.value,\n",
    "                diversity_weight=diversity_slider.value\n",
    "            )\n",
    "            context = \" \".join(context_chunks)\n",
    "            \n",
    "            # Invoke Claude model and display response\n",
    "            chat_output.clear_output()\n",
    "            display(Markdown(f\"**User Question:** {question}\"))\n",
    "            display(Markdown(\"*Generating answer with balanced source distribution...*\"))\n",
    "            \n",
    "            response = invoke_claude_model_balanced(context, question, max_output_tokens=6000)\n",
    "            \n",
    "            if response:\n",
    "                # Clear processing messages\n",
    "                chat_output.clear_output()\n",
    "                display(Markdown(f\"**User Question:** {question}\"))\n",
    "                \n",
    "                # Format the response with HTML for better readability\n",
    "                display(HTML(f\"<div style='white-space: pre-wrap; margin: 10px 0;'><b>AI Response:</b><br>{response}</div>\"))\n",
    "                \n",
    "                # Show source balance metrics\n",
    "                all_sources = set(chunk_doc_map)\n",
    "                source_coverage = (len(selected_sources) / len(all_sources)) * 100 if all_sources else 0\n",
    "                \n",
    "                display(HTML(f\"<div style='margin: 15px 0;'>\"\n",
    "                           f\"<b>Source Coverage:</b> {len(selected_sources)}/{len(all_sources)} sources \"\n",
    "                           f\"({source_coverage:.1f}%)</div>\"))\n",
    "                \n",
    "                if selected_sources:\n",
    "                    # Format detailed source usage statistics\n",
    "                    display(HTML(\"<b>Source Distribution:</b>\"))\n",
    "                    display(HTML(\"<div style='margin: 10px 0; padding: 10px; background-color: #f8f8f8; border-radius: 5px;'>\"))\n",
    "                    \n",
    "                    # Display table with source usage information\n",
    "                    table_html = \"\"\"<table style='width:100%; border-collapse: collapse;'>\n",
    "                        <tr style='background-color: #e0e0e0;'>\n",
    "                            <th style='padding: 8px; text-align: left; border: 1px solid #ddd;'>Source</th>\n",
    "                            <th style='padding: 8px; text-align: center; border: 1px solid #ddd;'>Chunks Used</th>\n",
    "                            <th style='padding: 8px; text-align: center; border: 1px solid #ddd;'>% of Context</th>\n",
    "                        </tr>\"\"\"\n",
    "                    \n",
    "                    # Calculate total chunks used\n",
    "                    total_chunks = sum(source_counter.values())\n",
    "                    \n",
    "                    # Add rows for each source\n",
    "                    for source, count in source_counter.most_common():\n",
    "                        percentage = (count / total_chunks) * 100\n",
    "                        table_html += f\"\"\"<tr>\n",
    "                            <td style='padding: 8px; border: 1px solid #ddd;'>{source}</td>\n",
    "                            <td style='padding: 8px; text-align: center; border: 1px solid #ddd;'>{count}</td>\n",
    "                            <td style='padding: 8px; text-align: center; border: 1px solid #ddd;'>{percentage:.1f}%</td>\n",
    "                        </tr>\"\"\"\n",
    "                    \n",
    "                    table_html += \"</table>\"\n",
    "                    display(HTML(table_html))\n",
    "                    display(HTML(\"</div>\"))\n",
    "                    \n",
    "                    # Add option to show all available sources\n",
    "                    show_all_button = widgets.Button(\n",
    "                        description=\"Show All Available Sources\",\n",
    "                        button_style=\"info\",\n",
    "                        layout=widgets.Layout(width='auto')\n",
    "                    )\n",
    "                    \n",
    "                    sources_output = widgets.Output()\n",
    "                    \n",
    "                    def on_show_all(b):\n",
    "                        with sources_output:\n",
    "                            sources_output.clear_output()\n",
    "                            unique_sources = set(chunk_doc_map)\n",
    "                            display(HTML(\"<b>All Available Sources:</b>\"))\n",
    "                            for src in unique_sources:\n",
    "                                has_chunks = src in source_counter\n",
    "                                style = \"color: green; font-weight: bold;\" if has_chunks else \"color: #888;\"\n",
    "                                count_text = f\"({source_counter[src]} chunks)\" if has_chunks else \"(not used)\"\n",
    "                                display(HTML(f\"<div style='margin-left: 20px; {style}'>• {src} {count_text}</div>\"))\n",
    "                    \n",
    "                    show_all_button.on_click(on_show_all)\n",
    "                    display(show_all_button)\n",
    "                    display(sources_output)\n",
    "            else:\n",
    "                display(Markdown(\"**Error:** Failed to get a response from the AI model.\"))\n",
    "        \n",
    "        # Clear the input field for the next question\n",
    "        question_input.value = ''\n",
    "\n",
    "    # Connect the button to the handler function\n",
    "    submit_button.on_click(on_submit_question)\n",
    "\n",
    "    # Handle Enter key in the text input\n",
    "    question_input.observe(lambda change: on_submit_question(None) \n",
    "                         if change['type'] == 'change' and change['name'] == 'value' and change.get('new', '').endswith('\\n') \n",
    "                         else None, 'value')\n",
    "\n",
    "    # Return the assembled interface\n",
    "    return widgets.VBox([\n",
    "        widgets.HTML(\"<h3>⚖️ Chat with Your Documents (Balanced Source Distribution)</h3>\"),\n",
    "        widgets.HBox([question_input, submit_button]),\n",
    "        widgets.HBox([chunk_slider, diversity_slider]),\n",
    "        chat_output\n",
    "    ])\n",
    "\n",
    "\n",
    "display(create_balanced_chat_interface()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
